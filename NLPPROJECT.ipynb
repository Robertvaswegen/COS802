{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONA+xkpeC45JtdLeLa7Z5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Robertvaswegen/COS802/blob/main/NLPPROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IKz9eTpioeQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "!pip install tweet-preprocessor bertopic nltk emoji transformers spacy geopy hdbscan umap-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install contractions\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "from geopy.geocoders import Nominatim\n",
        "from tqdm import tqdm\n",
        "import preprocessor as p\n",
        "import contractions\n",
        "import html"
      ],
      "metadata": {
        "id": "dDV9vBgsiqRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords_en = set(stopwords.words('english')).union({'like', 'just', 'get', 'got'})\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned = p.clean(text)\n",
        "    cleaned = contractions.fix(cleaned)\n",
        "    cleaned = re.sub(r'#', '', cleaned)\n",
        "    cleaned = re.sub(r'[^\\w\\s]', '', cleaned)\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "    cleaned = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", cleaned)\n",
        "    cleaned = re.sub(r\"@\\w+\", \"\", cleaned)\n",
        "    cleaned = html.unescape(cleaned)\n",
        "    return cleaned.lower().strip()\n",
        "\n",
        "\n",
        "def remove_stopwords_and_lemmatize(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords_en])\n",
        "\n",
        "\n",
        "data['text_clean'] = data['text'].apply(clean_text).apply(remove_stopwords_and_lemmatize)\n",
        "\n",
        "\n",
        "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.1, metric='cosine')\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "vectorizer_model = CountVectorizer(stop_words=list(stopwords_en), ngram_range=(1, 2))   # Including bigrams\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "ctfidf_model = ClassTfidfTransformer()\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    ctfidf_model=ctfidf_model,\n",
        "    representation_model=representation_model\n",
        ")\n",
        "\n",
        "\n",
        "data_en = data[data['lang'] == 'en'].copy()\n",
        "\n",
        "\n",
        "all_topics, all_probabilities = [], []\n",
        "batch_size = 50000\n",
        "\n",
        "for i in tqdm(range(0, len(data_en), batch_size)):\n",
        "    data_batch = data_en['text_clean'].iloc[i : i + batch_size]\n",
        "    topics, probabilities = topic_model.fit_transform(data_batch.tolist())\n",
        "    all_topics.extend(topics)\n",
        "    all_probabilities.extend(probabilities)\n",
        "\n",
        "\n",
        "data_en['topic'] = all_topics\n",
        "data_en['topic_probability'] = all_probabilities\n",
        "\n",
        "\n",
        "unclassified_tweets = data_en[data_en['topic'] == -1]\n",
        "\n",
        "\n",
        "data_en['topic'] = all_topics\n",
        "data_en['topic_probability'] = all_probabilities\n",
        "data_en.to_parquet('/content/drive/My Drive/sapoliceservice_topics_latest.parquet')\n"
      ],
      "metadata": {
        "id": "loTtA4-lishN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if len(unclassified_tweets) > 0:\n",
        "    unclassified_texts = unclassified_tweets['text_clean'].tolist()\n",
        "    refined_umap_model = UMAP(n_neighbors=5, n_components=5, min_dist=0.1, metric='cosine')\n",
        "    refined_hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='leaf')\n",
        "\n",
        "\n",
        "    refined_topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        umap_model=refined_umap_model,\n",
        "        hdbscan_model=refined_hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        ctfidf_model=ctfidf_model,\n",
        "        representation_model=representation_model\n",
        "    )\n",
        "\n",
        "\n",
        "    refined_topics_all, refined_probabilities_all = [], []\n",
        "    refined_batch_size = 10000\n",
        "\n",
        "    for i in tqdm(range(0, len(unclassified_texts), refined_batch_size)):\n",
        "        unclassified_batch = unclassified_texts[i : i + refined_batch_size]\n",
        "        refined_topics, refined_probabilities = refined_topic_model.fit_transform(unclassified_batch)\n",
        "        refined_topics_all.extend(refined_topics)\n",
        "        refined_probabilities_all.extend(refined_probabilities)\n",
        "\n",
        "\n",
        "    unclassified_tweets['refined_topic'] = refined_topics_all\n",
        "    unclassified_tweets['refined_topic_probability'] = refined_probabilities_all\n",
        "\n",
        "\n",
        "    unclassified_tweets.to_parquet('/content/drive/My Drive/sapoliceservice_topics_unclassified_tweets_latest.parquet')\n"
      ],
      "metadata": {
        "id": "NSVdBESLjAIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "processed_texts = data_en['text_clean'].apply(lambda x: x.split()).tolist()\n",
        "dictionary = Dictionary(processed_texts)\n",
        "\n",
        "\n",
        "topic_words = [\n",
        "    [word for word, _ in topic_model.get_topic(topic_id)]\n",
        "    for topic_id in topic_model.get_topics()\n",
        "    if topic_id != -1\n",
        "]\n",
        "\n",
        "\n",
        "filtered_topics = [topic for topic in topic_words if len(topic) > 0]\n",
        "\n",
        "if filtered_topics:\n",
        "    topic_words_ids = [\n",
        "        [dictionary.token2id[word] for word in topic if word in dictionary.token2id]\n",
        "        for topic in filtered_topics\n",
        "    ]\n",
        "\n",
        "    topic_words_ids = [topic for topic in topic_words_ids if len(topic) > 0]\n",
        "\n",
        "\n",
        "    if topic_words_ids:\n",
        "        coherence_model_npmi = CoherenceModel(\n",
        "            topics=topic_words_ids,\n",
        "            texts=processed_texts,\n",
        "            dictionary=dictionary,\n",
        "            coherence='c_npmi'\n",
        "        )\n",
        "        topic_coherence_npmi = coherence_model_npmi.get_coherence()\n",
        "\n",
        "        coherence_model_cv = CoherenceModel(\n",
        "            topics=topic_words_ids,\n",
        "            texts=processed_texts,\n",
        "            dictionary=dictionary,\n",
        "            coherence='c_v'\n",
        "        )\n",
        "        topic_coherence_cv = coherence_model_cv.get_coherence()\n",
        "\n",
        "\n",
        "        print(f\"Topic Coherence (NPMI): {topic_coherence_npmi if not np.isnan(topic_coherence_npmi) else 'N/A'}\")\n",
        "        print(f\"Topic Coherence (c_v): {topic_coherence_cv}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No valid topics found for coherence calculation.\")\n",
        "else:\n",
        "    print(\"No non-empty topics available.\")\n",
        "\n",
        "\n",
        "if filtered_topics:\n",
        "    unique_words = set(word for topic in filtered_topics for word in topic)\n",
        "    total_words = sum(len(topic) for topic in filtered_topics)\n",
        "    topic_diversity = len(unique_words) / total_words\n",
        "    print(f\"Topic Diversity: {topic_diversity}\")\n",
        "else:\n",
        "    print(\"No valid topics found for diversity calculation.\")\n"
      ],
      "metadata": {
        "id": "l3Vs97XAjJcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topic intertopic distance\n",
        "topic_model.visualize_topics()\n"
      ],
      "metadata": {
        "id": "sxmzzJNKjS3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart()"
      ],
      "metadata": {
        "id": "1C_dnh6RjUi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate and plot word clouds for each topic\n",
        "for topic_id in range(len(topic_words)):\n",
        "    words = \" \".join(topic_words[topic_id])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(words)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Topic {topic_id}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ufuHFfLEjWKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "dictionary = corpora.Dictionary([text.split() for text in data_en['text_clean']])\n",
        "\n",
        "\n",
        "corpus = [dictionary.doc2bow(text.split()) for text in data_en['text_clean']]\n",
        "\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=15, random_state=42)\n",
        "\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in data_en['text_clean']], dictionary=dictionary, coherence='c_v')\n",
        "\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "\n",
        "print(f\"Coherence Score (c_v): {coherence_score}\")\n"
      ],
      "metadata": {
        "id": "jlwQy6bNjai-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "lda_vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "\n",
        "pyLDAvis.display(lda_vis)\n",
        "\n"
      ],
      "metadata": {
        "id": "W2LPvkbnjjTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis with RoBERTa\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "all_sentiments, batch_size = [], 50000\n",
        "for i in tqdm(range(0, len(data_en), batch_size)):\n",
        "    batch = data_en['text_clean'].iloc[i:i + batch_size].tolist()\n",
        "    sentiments = sentiment_pipeline(batch, padding=True, truncation=True, max_length=128)\n",
        "    batch_sentiments = [result['label'] for result in sentiments]\n",
        "    all_sentiments.extend(batch_sentiments)\n",
        "\n",
        "data_en['sentiment'] = all_sentiments\n",
        "data_en.to_parquet('/content/drive/My Drive/sapoliceservice_tweets_with_sentiments.parquet')"
      ],
      "metadata": {
        "id": "SzaeTmW8jqlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "sentiment_output = pd.read_parquet('/content/drive/MyDrive/sapoliceservice_tweets_with_sentiments.parquet')\n",
        "\n",
        "\n",
        "label_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
        "sentiment_output['sentiment_label'] = sentiment_output['sentiment'].map(label_mapping)\n",
        "\n",
        "\n",
        "sentiment_output['predicted_sentiment_label'] = sentiment_output['sentiment'].map(label_mapping)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'])\n",
        "precision = precision_score(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'], average='weighted')\n",
        "recall = recall_score(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'], average='weighted')\n",
        "f1 = f1_score(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'], average='weighted')\n",
        "conf_matrix = confusion_matrix(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'])\n",
        "\n",
        "\n",
        "print(f\"Sentiment Analysis Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "id": "REd-UyDKjzzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_matrix = confusion_matrix(sentiment_output['sentiment_label'], sentiment_output['predicted_sentiment_label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zqCf8Y3Bj4nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = sentiment_output['sentiment'].value_counts()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"coolwarm\")\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qALYwjzekKv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_output['created_at'] = pd.to_datetime(sentiment_output['created_at'])\n",
        "sentiment_output.set_index('created_at', inplace=True)\n",
        "sentiment_over_time = sentiment_output.resample('W')['sentiment'].value_counts().unstack()\n",
        "sentiment_over_time.plot(kind='line', figsize=(12, 6))\n",
        "plt.title(\"Sentiment Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sentiment Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WHmlwAiSkMiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "subset_tweets = data_en.sample(n=100000, random_state=18014519)\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def extract_location_and_geocode(text):\n",
        "    doc = nlp(text)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
        "    if locations:\n",
        "        for location in locations:\n",
        "            try:\n",
        "                geo_info = geolocator.geocode(location, timeout=10)\n",
        "                if geo_info:\n",
        "                    return geo_info.latitude, geo_info.longitude, location\n",
        "            except Exception as e:\n",
        "                print(f\"Error geocoding {location}: {e}\")\n",
        "                continue\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "subset_tweets[['latitude', 'longitude', 'extracted_location']] = subset_tweets['text_clean'].progress_apply(\n",
        "    lambda x: pd.Series(extract_location_and_geocode(x))\n",
        ")\n",
        "\n",
        "subset_tweets.to_parquet('/content/drive/My Drive/sapoliceservice_subset_with_locations.parquet')\n"
      ],
      "metadata": {
        "id": "_O2tZHiWj8Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "# Initialize map centered on South Africa (or appropriate center)\n",
        "m = folium.Map(location=[-30.5595, 22.9375], zoom_start=5)\n",
        "\n",
        "# Add markers for each tweet with geolocation\n",
        "for _, row in geolocation_output.dropna(subset=['latitude', 'longitude']).iterrows():\n",
        "    folium.Marker([row['latitude'], row['longitude']], popup=row['extracted_location']).add_to(m)\n",
        "\n",
        "# Save the map as an HTML file or display in a notebook\n",
        "m.save(\"geolocation_map.html\")\n",
        "m\n"
      ],
      "metadata": {
        "id": "XhdiQCHYkO7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}